{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../dataset/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../dataset/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../dataset/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../dataset/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('../dataset/MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_size = 64\n",
    "Z_dim = 100\n",
    "X_dim = mnist.train.images.shape[1]\n",
    "y_dim = mnist.train.labels.shape[1]\n",
    "h_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = 0\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, batch_size, input_dim, h_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.W1 = nn.Parameter(torch.randn(input_dim, h_dim)*0.075)\n",
    "        self.b1 = nn.Parameter(torch.randn(h_dim)*0.075)\n",
    "        self.W2 = nn.Parameter(torch.randn(h_dim, output_dim)*0.075)\n",
    "        self.b2 = nn.Parameter(torch.randn(output_dim)*0.075)\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = F.relu(X.mm(self.W1) + self.b1.repeat(X.size(0), 1))\n",
    "        z = h.mm(self.W2) + self.b2.repeat(h.size(0), 1)\n",
    "        return F.sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = SimpleNN(mb_size, Z_dim, h_dim, X_dim)\n",
    "D = SimpleNN(mb_size, X_dim, h_dim, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G_solver = optim.Adam(G.parameters(), lr=1e-3)\n",
    "D_solver = optim.Adam(D.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ones_label = Variable(torch.ones(mb_size))\n",
    "zeros_label = Variable(torch.zeros(mb_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = F.binary_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: D loss: 1.6841; G loss: 0.4389\n",
      "step 100: D loss: 1.6987; G loss: 0.4409\n",
      "step 200: D loss: 1.6832; G loss: 0.4391\n",
      "step 300: D loss: 1.6618; G loss: 0.4418\n",
      "step 400: D loss: 1.7100; G loss: 0.4398\n",
      "step 500: D loss: 1.6740; G loss: 0.4383\n",
      "step 600: D loss: 1.6775; G loss: 0.4353\n",
      "step 700: D loss: 1.6856; G loss: 0.4326\n",
      "step 800: D loss: 1.6882; G loss: 0.4305\n",
      "step 900: D loss: 1.7156; G loss: 0.4374\n",
      "step 1000: D loss: 1.6516; G loss: 0.4410\n",
      "step 1100: D loss: 1.6942; G loss: 0.4353\n",
      "step 1200: D loss: 1.6657; G loss: 0.4378\n",
      "step 1300: D loss: 1.7077; G loss: 0.4353\n",
      "step 1400: D loss: 1.6877; G loss: 0.4388\n",
      "step 1500: D loss: 1.7050; G loss: 0.4365\n",
      "step 1600: D loss: 1.6939; G loss: 0.4260\n",
      "step 1700: D loss: 1.7105; G loss: 0.4433\n",
      "step 1800: D loss: 1.6713; G loss: 0.4424\n",
      "step 1900: D loss: 1.6625; G loss: 0.4439\n",
      "step 2000: D loss: 1.7031; G loss: 0.4386\n",
      "step 2100: D loss: 1.7055; G loss: 0.4437\n",
      "step 2200: D loss: 1.6950; G loss: 0.4401\n",
      "step 2300: D loss: 1.7018; G loss: 0.4334\n",
      "step 2400: D loss: 1.6832; G loss: 0.4377\n",
      "step 2500: D loss: 1.7112; G loss: 0.4367\n",
      "step 2600: D loss: 1.7188; G loss: 0.4486\n",
      "step 2700: D loss: 1.7117; G loss: 0.4318\n",
      "step 2800: D loss: 1.6910; G loss: 0.4355\n",
      "step 2900: D loss: 1.7208; G loss: 0.4363\n",
      "step 3000: D loss: 1.7088; G loss: 0.4286\n",
      "step 3100: D loss: 1.6733; G loss: 0.4392\n",
      "step 3200: D loss: 1.6826; G loss: 0.4406\n",
      "step 3300: D loss: 1.6768; G loss: 0.4352\n",
      "step 3400: D loss: 1.6978; G loss: 0.4370\n",
      "step 3500: D loss: 1.6874; G loss: 0.4345\n",
      "step 3600: D loss: 1.6816; G loss: 0.4394\n",
      "step 3700: D loss: 1.6857; G loss: 0.4372\n",
      "step 3800: D loss: 1.6925; G loss: 0.4404\n",
      "step 3900: D loss: 1.7028; G loss: 0.4317\n",
      "step 4000: D loss: 1.6907; G loss: 0.4363\n",
      "step 4100: D loss: 1.7029; G loss: 0.4306\n",
      "step 4200: D loss: 1.6933; G loss: 0.4345\n",
      "step 4300: D loss: 1.7121; G loss: 0.4349\n",
      "step 4400: D loss: 1.6884; G loss: 0.4380\n",
      "step 4500: D loss: 1.7024; G loss: 0.4345\n",
      "step 4600: D loss: 1.6988; G loss: 0.4342\n",
      "step 4700: D loss: 1.6954; G loss: 0.4384\n",
      "step 4800: D loss: 1.7038; G loss: 0.4282\n",
      "step 4900: D loss: 1.7161; G loss: 0.4380\n",
      "step 5000: D loss: 1.7177; G loss: 0.4333\n",
      "step 5100: D loss: 1.6969; G loss: 0.4311\n",
      "step 5200: D loss: 1.7126; G loss: 0.4341\n",
      "step 5300: D loss: 1.6802; G loss: 0.4427\n",
      "step 5400: D loss: 1.7050; G loss: 0.4303\n",
      "step 5500: D loss: 1.6863; G loss: 0.4421\n",
      "step 5600: D loss: 1.6605; G loss: 0.4366\n",
      "step 5700: D loss: 1.6949; G loss: 0.4318\n",
      "step 5800: D loss: 1.6786; G loss: 0.4403\n",
      "step 5900: D loss: 1.7056; G loss: 0.4407\n",
      "step 6000: D loss: 1.6639; G loss: 0.4400\n",
      "step 6100: D loss: 1.6953; G loss: 0.4367\n",
      "step 6200: D loss: 1.7023; G loss: 0.4301\n",
      "step 6300: D loss: 1.6746; G loss: 0.4426\n",
      "step 6400: D loss: 1.6980; G loss: 0.4370\n",
      "step 6500: D loss: 1.6709; G loss: 0.4410\n",
      "step 6600: D loss: 1.7324; G loss: 0.4411\n",
      "step 6700: D loss: 1.6837; G loss: 0.4308\n",
      "step 6800: D loss: 1.6797; G loss: 0.4340\n",
      "step 6900: D loss: 1.7281; G loss: 0.4427\n",
      "step 7000: D loss: 1.6764; G loss: 0.4492\n",
      "step 7100: D loss: 1.6777; G loss: 0.4444\n",
      "step 7200: D loss: 1.6659; G loss: 0.4297\n",
      "step 7300: D loss: 1.6986; G loss: 0.4354\n",
      "step 7400: D loss: 1.6832; G loss: 0.4376\n",
      "step 7500: D loss: 1.6802; G loss: 0.4462\n",
      "step 7600: D loss: 1.6643; G loss: 0.4300\n",
      "step 7700: D loss: 1.6566; G loss: 0.4365\n",
      "step 7800: D loss: 1.6910; G loss: 0.4321\n",
      "step 7900: D loss: 1.7225; G loss: 0.4370\n",
      "step 8000: D loss: 1.6855; G loss: 0.4355\n",
      "step 8100: D loss: 1.6772; G loss: 0.4366\n",
      "step 8200: D loss: 1.6865; G loss: 0.4386\n",
      "step 8300: D loss: 1.7039; G loss: 0.4355\n",
      "step 8400: D loss: 1.6870; G loss: 0.4392\n",
      "step 8500: D loss: 1.6617; G loss: 0.4386\n",
      "step 8600: D loss: 1.6940; G loss: 0.4338\n",
      "step 8700: D loss: 1.7024; G loss: 0.4375\n",
      "step 8800: D loss: 1.6677; G loss: 0.4353\n",
      "step 8900: D loss: 1.7140; G loss: 0.4402\n",
      "step 9000: D loss: 1.6926; G loss: 0.4353\n",
      "step 9100: D loss: 1.6913; G loss: 0.4331\n",
      "step 9200: D loss: 1.6922; G loss: 0.4347\n",
      "step 9300: D loss: 1.7068; G loss: 0.4375\n",
      "step 9400: D loss: 1.7096; G loss: 0.4347\n",
      "step 9500: D loss: 1.6747; G loss: 0.4377\n",
      "step 9600: D loss: 1.7087; G loss: 0.4516\n",
      "step 9700: D loss: 1.6819; G loss: 0.4307\n",
      "step 9800: D loss: 1.6813; G loss: 0.4357\n",
      "step 9900: D loss: 1.6727; G loss: 0.4358\n",
      "step 10000: D loss: 1.6784; G loss: 0.4475\n",
      "step 10100: D loss: 1.7093; G loss: 0.4382\n",
      "step 10200: D loss: 1.6647; G loss: 0.4384\n",
      "step 10300: D loss: 1.6971; G loss: 0.4428\n",
      "step 10400: D loss: 1.6705; G loss: 0.4408\n",
      "step 10500: D loss: 1.7189; G loss: 0.4370\n",
      "step 10600: D loss: 1.7024; G loss: 0.4376\n",
      "step 10700: D loss: 1.6750; G loss: 0.4330\n",
      "step 10800: D loss: 1.6859; G loss: 0.4314\n",
      "step 10900: D loss: 1.7118; G loss: 0.4423\n",
      "step 11000: D loss: 1.6753; G loss: 0.4393\n",
      "step 11100: D loss: 1.6736; G loss: 0.4303\n",
      "step 11200: D loss: 1.7042; G loss: 0.4442\n",
      "step 11300: D loss: 1.6653; G loss: 0.4401\n",
      "step 11400: D loss: 1.6799; G loss: 0.4405\n",
      "step 11500: D loss: 1.7327; G loss: 0.4349\n",
      "step 11600: D loss: 1.7079; G loss: 0.4322\n",
      "step 11700: D loss: 1.7001; G loss: 0.4396\n",
      "step 11800: D loss: 1.6879; G loss: 0.4414\n",
      "step 11900: D loss: 1.6808; G loss: 0.4368\n",
      "step 12000: D loss: 1.7074; G loss: 0.4360\n",
      "step 12100: D loss: 1.7106; G loss: 0.4403\n",
      "step 12200: D loss: 1.7105; G loss: 0.4409\n",
      "step 12300: D loss: 1.6688; G loss: 0.4418\n",
      "step 12400: D loss: 1.6929; G loss: 0.4494\n",
      "step 12500: D loss: 1.6558; G loss: 0.4372\n",
      "step 12600: D loss: 1.6871; G loss: 0.4290\n",
      "step 12700: D loss: 1.6850; G loss: 0.4310\n",
      "step 12800: D loss: 1.6508; G loss: 0.4338\n",
      "step 12900: D loss: 1.6967; G loss: 0.4423\n",
      "step 13000: D loss: 1.6953; G loss: 0.4333\n",
      "step 13100: D loss: 1.7065; G loss: 0.4381\n",
      "step 13200: D loss: 1.6857; G loss: 0.4391\n",
      "step 13300: D loss: 1.7006; G loss: 0.4383\n",
      "step 13400: D loss: 1.7058; G loss: 0.4443\n",
      "step 13500: D loss: 1.7024; G loss: 0.4330\n",
      "step 13600: D loss: 1.7433; G loss: 0.4440\n",
      "step 13700: D loss: 1.6885; G loss: 0.4353\n",
      "step 13800: D loss: 1.6848; G loss: 0.4437\n",
      "step 13900: D loss: 1.7388; G loss: 0.4358\n",
      "step 14000: D loss: 1.6924; G loss: 0.4346\n",
      "step 14100: D loss: 1.7004; G loss: 0.4360\n",
      "step 14200: D loss: 1.6282; G loss: 0.4386\n",
      "step 14300: D loss: 1.6947; G loss: 0.4357\n",
      "step 14400: D loss: 1.6982; G loss: 0.4416\n",
      "step 14500: D loss: 1.7176; G loss: 0.4390\n",
      "step 14600: D loss: 1.7071; G loss: 0.4325\n",
      "step 14700: D loss: 1.7016; G loss: 0.4354\n",
      "step 14800: D loss: 1.6793; G loss: 0.4456\n",
      "step 14900: D loss: 1.7117; G loss: 0.4351\n",
      "step 15000: D loss: 1.7050; G loss: 0.4354\n",
      "step 15100: D loss: 1.6813; G loss: 0.4372\n",
      "step 15200: D loss: 1.6647; G loss: 0.4397\n",
      "step 15300: D loss: 1.6993; G loss: 0.4274\n",
      "step 15400: D loss: 1.7083; G loss: 0.4415\n",
      "step 15500: D loss: 1.6769; G loss: 0.4323\n",
      "step 15600: D loss: 1.6699; G loss: 0.4466\n",
      "step 15700: D loss: 1.6832; G loss: 0.4408\n",
      "step 15800: D loss: 1.6917; G loss: 0.4318\n",
      "step 15900: D loss: 1.6880; G loss: 0.4352\n",
      "step 16000: D loss: 1.6914; G loss: 0.4349\n",
      "step 16100: D loss: 1.7158; G loss: 0.4407\n",
      "step 16200: D loss: 1.6788; G loss: 0.4359\n",
      "step 16300: D loss: 1.7006; G loss: 0.4353\n",
      "step 16400: D loss: 1.6635; G loss: 0.4310\n",
      "step 16500: D loss: 1.6754; G loss: 0.4373\n",
      "step 16600: D loss: 1.7166; G loss: 0.4308\n",
      "step 16700: D loss: 1.6821; G loss: 0.4386\n",
      "step 16800: D loss: 1.6756; G loss: 0.4444\n",
      "step 16900: D loss: 1.6897; G loss: 0.4409\n",
      "step 17000: D loss: 1.6856; G loss: 0.4392\n",
      "step 17100: D loss: 1.6990; G loss: 0.4407\n",
      "step 17200: D loss: 1.6640; G loss: 0.4404\n",
      "step 17300: D loss: 1.6756; G loss: 0.4357\n",
      "step 17400: D loss: 1.6797; G loss: 0.4462\n",
      "step 17500: D loss: 1.6839; G loss: 0.4334\n",
      "step 17600: D loss: 1.7165; G loss: 0.4386\n",
      "step 17700: D loss: 1.7229; G loss: 0.4393\n",
      "step 17800: D loss: 1.6788; G loss: 0.4432\n",
      "step 17900: D loss: 1.6879; G loss: 0.4425\n",
      "step 18000: D loss: 1.7002; G loss: 0.4371\n",
      "step 18100: D loss: 1.7030; G loss: 0.4348\n",
      "step 18200: D loss: 1.7023; G loss: 0.4393\n",
      "step 18300: D loss: 1.6849; G loss: 0.4307\n",
      "step 18400: D loss: 1.6676; G loss: 0.4386\n",
      "step 18500: D loss: 1.7060; G loss: 0.4349\n",
      "step 18600: D loss: 1.6635; G loss: 0.4404\n",
      "step 18700: D loss: 1.6874; G loss: 0.4389\n",
      "step 18800: D loss: 1.6870; G loss: 0.4383\n",
      "step 18900: D loss: 1.6892; G loss: 0.4443\n",
      "step 19000: D loss: 1.6691; G loss: 0.4442\n",
      "step 19100: D loss: 1.7092; G loss: 0.4358\n",
      "step 19200: D loss: 1.6845; G loss: 0.4331\n",
      "step 19300: D loss: 1.6697; G loss: 0.4336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 19400: D loss: 1.6865; G loss: 0.4342\n",
      "step 19500: D loss: 1.6598; G loss: 0.4425\n",
      "step 19600: D loss: 1.6825; G loss: 0.4319\n",
      "step 19700: D loss: 1.7106; G loss: 0.4364\n",
      "step 19800: D loss: 1.7030; G loss: 0.4356\n",
      "step 19900: D loss: 1.6930; G loss: 0.4383\n",
      "step 20000: D loss: 1.6928; G loss: 0.4385\n",
      "step 20100: D loss: 1.6851; G loss: 0.4404\n",
      "step 20200: D loss: 1.6905; G loss: 0.4403\n",
      "step 20300: D loss: 1.7011; G loss: 0.4334\n",
      "step 20400: D loss: 1.6989; G loss: 0.4368\n",
      "step 20500: D loss: 1.6956; G loss: 0.4351\n",
      "step 20600: D loss: 1.6953; G loss: 0.4378\n",
      "step 20700: D loss: 1.6723; G loss: 0.4458\n",
      "step 20800: D loss: 1.6885; G loss: 0.4318\n",
      "step 20900: D loss: 1.6805; G loss: 0.4336\n",
      "step 21000: D loss: 1.6653; G loss: 0.4312\n",
      "step 21100: D loss: 1.7115; G loss: 0.4324\n",
      "step 21200: D loss: 1.6971; G loss: 0.4458\n",
      "step 21300: D loss: 1.7079; G loss: 0.4334\n",
      "step 21400: D loss: 1.7076; G loss: 0.4471\n",
      "step 21500: D loss: 1.6929; G loss: 0.4421\n",
      "step 21600: D loss: 1.7145; G loss: 0.4394\n",
      "step 21700: D loss: 1.6587; G loss: 0.4350\n",
      "step 21800: D loss: 1.6684; G loss: 0.4283\n",
      "step 21900: D loss: 1.7016; G loss: 0.4392\n",
      "step 22000: D loss: 1.6665; G loss: 0.4443\n",
      "step 22100: D loss: 1.6908; G loss: 0.4362\n",
      "step 22200: D loss: 1.6777; G loss: 0.4317\n",
      "step 22300: D loss: 1.6832; G loss: 0.4424\n",
      "step 22400: D loss: 1.6753; G loss: 0.4351\n",
      "step 22500: D loss: 1.6970; G loss: 0.4299\n",
      "step 22600: D loss: 1.7095; G loss: 0.4415\n",
      "step 22700: D loss: 1.7041; G loss: 0.4367\n",
      "step 22800: D loss: 1.7103; G loss: 0.4373\n",
      "step 22900: D loss: 1.6739; G loss: 0.4373\n",
      "step 23000: D loss: 1.6846; G loss: 0.4434\n",
      "step 23100: D loss: 1.6864; G loss: 0.4418\n",
      "step 23200: D loss: 1.6982; G loss: 0.4394\n",
      "step 23300: D loss: 1.6999; G loss: 0.4389\n",
      "step 23400: D loss: 1.7078; G loss: 0.4400\n",
      "step 23500: D loss: 1.6776; G loss: 0.4429\n",
      "step 23600: D loss: 1.6663; G loss: 0.4462\n",
      "step 23700: D loss: 1.6931; G loss: 0.4369\n",
      "step 23800: D loss: 1.6755; G loss: 0.4317\n",
      "step 23900: D loss: 1.6771; G loss: 0.4348\n",
      "step 24000: D loss: 1.7145; G loss: 0.4411\n",
      "step 24100: D loss: 1.6862; G loss: 0.4299\n",
      "step 24200: D loss: 1.7223; G loss: 0.4450\n",
      "step 24300: D loss: 1.6886; G loss: 0.4415\n",
      "step 24400: D loss: 1.6913; G loss: 0.4402\n",
      "step 24500: D loss: 1.6908; G loss: 0.4327\n",
      "step 24600: D loss: 1.6583; G loss: 0.4382\n",
      "step 24700: D loss: 1.7017; G loss: 0.4344\n",
      "step 24800: D loss: 1.7170; G loss: 0.4468\n",
      "step 24900: D loss: 1.6967; G loss: 0.4378\n",
      "step 25000: D loss: 1.7137; G loss: 0.4372\n",
      "step 25100: D loss: 1.6939; G loss: 0.4427\n",
      "step 25200: D loss: 1.7045; G loss: 0.4368\n",
      "step 25300: D loss: 1.6826; G loss: 0.4347\n",
      "step 25400: D loss: 1.6973; G loss: 0.4412\n",
      "step 25500: D loss: 1.6825; G loss: 0.4408\n",
      "step 25600: D loss: 1.7044; G loss: 0.4393\n",
      "step 25700: D loss: 1.6857; G loss: 0.4436\n",
      "step 25800: D loss: 1.6714; G loss: 0.4448\n",
      "step 25900: D loss: 1.6946; G loss: 0.4299\n",
      "step 26000: D loss: 1.7229; G loss: 0.4313\n",
      "step 26100: D loss: 1.6838; G loss: 0.4421\n",
      "step 26200: D loss: 1.6804; G loss: 0.4478\n",
      "step 26300: D loss: 1.6838; G loss: 0.4415\n",
      "step 26400: D loss: 1.6507; G loss: 0.4418\n",
      "step 26500: D loss: 1.6852; G loss: 0.4400\n",
      "step 26600: D loss: 1.7004; G loss: 0.4361\n",
      "step 26700: D loss: 1.6873; G loss: 0.4335\n",
      "step 26800: D loss: 1.6959; G loss: 0.4357\n",
      "step 26900: D loss: 1.6971; G loss: 0.4433\n",
      "step 27000: D loss: 1.6836; G loss: 0.4420\n",
      "step 27100: D loss: 1.6877; G loss: 0.4298\n",
      "step 27200: D loss: 1.6982; G loss: 0.4320\n",
      "step 27300: D loss: 1.6753; G loss: 0.4425\n",
      "step 27400: D loss: 1.7341; G loss: 0.4385\n",
      "step 27500: D loss: 1.6872; G loss: 0.4401\n",
      "step 27600: D loss: 1.7267; G loss: 0.4353\n",
      "step 27700: D loss: 1.7159; G loss: 0.4363\n",
      "step 27800: D loss: 1.6772; G loss: 0.4416\n",
      "step 27900: D loss: 1.6800; G loss: 0.4382\n",
      "step 28000: D loss: 1.6999; G loss: 0.4400\n",
      "step 28100: D loss: 1.7139; G loss: 0.4352\n",
      "step 28200: D loss: 1.7030; G loss: 0.4434\n",
      "step 28300: D loss: 1.6537; G loss: 0.4411\n",
      "step 28400: D loss: 1.7187; G loss: 0.4443\n",
      "step 28500: D loss: 1.6835; G loss: 0.4335\n",
      "step 28600: D loss: 1.7301; G loss: 0.4323\n",
      "step 28700: D loss: 1.7165; G loss: 0.4403\n",
      "step 28800: D loss: 1.6728; G loss: 0.4384\n",
      "step 28900: D loss: 1.6923; G loss: 0.4361\n",
      "step 29000: D loss: 1.6949; G loss: 0.4306\n",
      "step 29100: D loss: 1.6688; G loss: 0.4325\n",
      "step 29200: D loss: 1.7055; G loss: 0.4390\n",
      "step 29300: D loss: 1.6811; G loss: 0.4400\n",
      "step 29400: D loss: 1.6928; G loss: 0.4376\n",
      "step 29500: D loss: 1.6985; G loss: 0.4354\n",
      "step 29600: D loss: 1.7236; G loss: 0.4367\n",
      "step 29700: D loss: 1.6882; G loss: 0.4446\n",
      "step 29800: D loss: 1.6606; G loss: 0.4475\n",
      "step 29900: D loss: 1.7144; G loss: 0.4282\n",
      "step 30000: D loss: 1.6720; G loss: 0.4358\n",
      "step 30100: D loss: 1.6859; G loss: 0.4364\n",
      "step 30200: D loss: 1.6406; G loss: 0.4407\n",
      "step 30300: D loss: 1.6719; G loss: 0.4365\n",
      "step 30400: D loss: 1.6974; G loss: 0.4356\n",
      "step 30500: D loss: 1.6939; G loss: 0.4347\n",
      "step 30600: D loss: 1.6954; G loss: 0.4343\n",
      "step 30700: D loss: 1.6865; G loss: 0.4356\n",
      "step 30800: D loss: 1.6712; G loss: 0.4448\n",
      "step 30900: D loss: 1.6978; G loss: 0.4320\n",
      "step 31000: D loss: 1.6915; G loss: 0.4386\n",
      "step 31100: D loss: 1.6824; G loss: 0.4387\n",
      "step 31200: D loss: 1.6939; G loss: 0.4453\n",
      "step 31300: D loss: 1.6744; G loss: 0.4276\n",
      "step 31400: D loss: 1.6855; G loss: 0.4501\n",
      "step 31500: D loss: 1.6750; G loss: 0.4315\n",
      "step 31600: D loss: 1.6940; G loss: 0.4402\n",
      "step 31700: D loss: 1.6540; G loss: 0.4388\n",
      "step 31800: D loss: 1.6827; G loss: 0.4409\n",
      "step 31900: D loss: 1.7046; G loss: 0.4299\n",
      "step 32000: D loss: 1.6977; G loss: 0.4374\n",
      "step 32100: D loss: 1.6736; G loss: 0.4428\n",
      "step 32200: D loss: 1.6830; G loss: 0.4342\n",
      "step 32300: D loss: 1.6969; G loss: 0.4331\n",
      "step 32400: D loss: 1.7042; G loss: 0.4418\n",
      "step 32500: D loss: 1.6985; G loss: 0.4370\n",
      "step 32600: D loss: 1.6902; G loss: 0.4364\n",
      "step 32700: D loss: 1.6795; G loss: 0.4399\n",
      "step 32800: D loss: 1.7037; G loss: 0.4414\n",
      "step 32900: D loss: 1.6807; G loss: 0.4337\n",
      "step 33000: D loss: 1.6629; G loss: 0.4339\n",
      "step 33100: D loss: 1.6917; G loss: 0.4374\n",
      "step 33200: D loss: 1.6712; G loss: 0.4337\n",
      "step 33300: D loss: 1.6966; G loss: 0.4248\n",
      "step 33400: D loss: 1.7369; G loss: 0.4356\n",
      "step 33500: D loss: 1.6574; G loss: 0.4293\n",
      "step 33600: D loss: 1.6760; G loss: 0.4355\n",
      "step 33700: D loss: 1.6974; G loss: 0.4383\n",
      "step 33800: D loss: 1.6852; G loss: 0.4440\n",
      "step 33900: D loss: 1.6842; G loss: 0.4394\n",
      "step 34000: D loss: 1.6690; G loss: 0.4408\n",
      "step 34100: D loss: 1.6946; G loss: 0.4295\n",
      "step 34200: D loss: 1.7184; G loss: 0.4399\n",
      "step 34300: D loss: 1.6781; G loss: 0.4341\n",
      "step 34400: D loss: 1.7386; G loss: 0.4380\n",
      "step 34500: D loss: 1.6984; G loss: 0.4390\n",
      "step 34600: D loss: 1.7061; G loss: 0.4388\n",
      "step 34700: D loss: 1.6905; G loss: 0.4276\n",
      "step 34800: D loss: 1.6990; G loss: 0.4364\n",
      "step 34900: D loss: 1.7173; G loss: 0.4387\n",
      "step 35000: D loss: 1.6514; G loss: 0.4376\n",
      "step 35100: D loss: 1.7298; G loss: 0.4322\n",
      "step 35200: D loss: 1.7086; G loss: 0.4442\n",
      "step 35300: D loss: 1.6911; G loss: 0.4288\n",
      "step 35400: D loss: 1.6824; G loss: 0.4359\n",
      "step 35500: D loss: 1.7243; G loss: 0.4381\n",
      "step 35600: D loss: 1.6670; G loss: 0.4312\n",
      "step 35700: D loss: 1.7093; G loss: 0.4403\n",
      "step 35800: D loss: 1.6846; G loss: 0.4377\n",
      "step 35900: D loss: 1.6942; G loss: 0.4395\n",
      "step 36000: D loss: 1.6763; G loss: 0.4364\n",
      "step 36100: D loss: 1.6550; G loss: 0.4394\n",
      "step 36200: D loss: 1.7154; G loss: 0.4352\n",
      "step 36300: D loss: 1.6789; G loss: 0.4321\n",
      "step 36400: D loss: 1.6961; G loss: 0.4521\n",
      "step 36500: D loss: 1.6731; G loss: 0.4389\n",
      "step 36600: D loss: 1.7204; G loss: 0.4375\n",
      "step 36700: D loss: 1.6972; G loss: 0.4386\n",
      "step 36800: D loss: 1.6752; G loss: 0.4354\n",
      "step 36900: D loss: 1.6672; G loss: 0.4383\n",
      "step 37000: D loss: 1.6853; G loss: 0.4235\n",
      "step 37100: D loss: 1.7051; G loss: 0.4358\n",
      "step 37200: D loss: 1.6636; G loss: 0.4371\n",
      "step 37300: D loss: 1.6726; G loss: 0.4414\n",
      "step 37400: D loss: 1.6941; G loss: 0.4417\n",
      "step 37500: D loss: 1.6738; G loss: 0.4323\n",
      "step 37600: D loss: 1.6822; G loss: 0.4415\n",
      "step 37700: D loss: 1.7029; G loss: 0.4389\n",
      "step 37800: D loss: 1.7132; G loss: 0.4323\n",
      "step 37900: D loss: 1.6778; G loss: 0.4376\n",
      "step 38000: D loss: 1.6977; G loss: 0.4359\n",
      "step 38100: D loss: 1.7019; G loss: 0.4422\n",
      "step 38200: D loss: 1.7096; G loss: 0.4325\n",
      "step 38300: D loss: 1.6548; G loss: 0.4275\n",
      "step 38400: D loss: 1.6710; G loss: 0.4373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 38500: D loss: 1.7055; G loss: 0.4385\n",
      "step 38600: D loss: 1.6714; G loss: 0.4378\n",
      "step 38700: D loss: 1.6331; G loss: 0.4479\n",
      "step 38800: D loss: 1.6929; G loss: 0.4410\n",
      "step 38900: D loss: 1.6730; G loss: 0.4395\n",
      "step 39000: D loss: 1.7146; G loss: 0.4355\n",
      "step 39100: D loss: 1.6707; G loss: 0.4342\n",
      "step 39200: D loss: 1.6843; G loss: 0.4479\n",
      "step 39300: D loss: 1.6649; G loss: 0.4490\n",
      "step 39400: D loss: 1.7088; G loss: 0.4390\n",
      "step 39500: D loss: 1.7186; G loss: 0.4440\n",
      "step 39600: D loss: 1.7247; G loss: 0.4342\n",
      "step 39700: D loss: 1.6866; G loss: 0.4278\n",
      "step 39800: D loss: 1.6966; G loss: 0.4351\n",
      "step 39900: D loss: 1.6765; G loss: 0.4338\n",
      "step 40000: D loss: 1.7107; G loss: 0.4371\n",
      "step 40100: D loss: 1.7273; G loss: 0.4352\n",
      "step 40200: D loss: 1.7134; G loss: 0.4389\n",
      "step 40300: D loss: 1.6820; G loss: 0.4418\n",
      "step 40400: D loss: 1.6696; G loss: 0.4455\n",
      "step 40500: D loss: 1.6772; G loss: 0.4303\n",
      "step 40600: D loss: 1.7262; G loss: 0.4424\n",
      "step 40700: D loss: 1.7065; G loss: 0.4393\n",
      "step 40800: D loss: 1.7190; G loss: 0.4421\n",
      "step 40900: D loss: 1.6873; G loss: 0.4389\n",
      "step 41000: D loss: 1.6960; G loss: 0.4428\n",
      "step 41100: D loss: 1.6873; G loss: 0.4354\n",
      "step 41200: D loss: 1.6831; G loss: 0.4332\n",
      "step 41300: D loss: 1.6973; G loss: 0.4402\n",
      "step 41400: D loss: 1.6998; G loss: 0.4343\n",
      "step 41500: D loss: 1.7013; G loss: 0.4442\n",
      "step 41600: D loss: 1.7122; G loss: 0.4350\n",
      "step 41700: D loss: 1.6838; G loss: 0.4410\n",
      "step 41800: D loss: 1.6772; G loss: 0.4358\n",
      "step 41900: D loss: 1.6747; G loss: 0.4354\n",
      "step 42000: D loss: 1.7033; G loss: 0.4337\n",
      "step 42100: D loss: 1.7099; G loss: 0.4367\n",
      "step 42200: D loss: 1.7011; G loss: 0.4354\n",
      "step 42300: D loss: 1.6970; G loss: 0.4389\n",
      "step 42400: D loss: 1.6819; G loss: 0.4419\n",
      "step 42500: D loss: 1.6993; G loss: 0.4293\n",
      "step 42600: D loss: 1.6838; G loss: 0.4355\n",
      "step 42700: D loss: 1.7093; G loss: 0.4292\n",
      "step 42800: D loss: 1.6629; G loss: 0.4400\n",
      "step 42900: D loss: 1.6933; G loss: 0.4336\n",
      "step 43000: D loss: 1.6971; G loss: 0.4306\n",
      "step 43100: D loss: 1.7039; G loss: 0.4446\n",
      "step 43200: D loss: 1.7144; G loss: 0.4371\n",
      "step 43300: D loss: 1.7047; G loss: 0.4455\n",
      "step 43400: D loss: 1.6737; G loss: 0.4391\n",
      "step 43500: D loss: 1.7077; G loss: 0.4393\n",
      "step 43600: D loss: 1.6983; G loss: 0.4332\n",
      "step 43700: D loss: 1.6764; G loss: 0.4366\n",
      "step 43800: D loss: 1.6655; G loss: 0.4356\n",
      "step 43900: D loss: 1.6981; G loss: 0.4352\n",
      "step 44000: D loss: 1.6962; G loss: 0.4313\n",
      "step 44100: D loss: 1.6746; G loss: 0.4382\n",
      "step 44200: D loss: 1.6870; G loss: 0.4394\n",
      "step 44300: D loss: 1.6683; G loss: 0.4414\n",
      "step 44400: D loss: 1.7048; G loss: 0.4348\n",
      "step 44500: D loss: 1.6991; G loss: 0.4353\n",
      "step 44600: D loss: 1.6695; G loss: 0.4397\n",
      "step 44700: D loss: 1.6657; G loss: 0.4328\n",
      "step 44800: D loss: 1.6664; G loss: 0.4385\n",
      "step 44900: D loss: 1.7120; G loss: 0.4405\n",
      "step 45000: D loss: 1.6785; G loss: 0.4422\n",
      "step 45100: D loss: 1.6679; G loss: 0.4346\n",
      "step 45200: D loss: 1.7085; G loss: 0.4323\n",
      "step 45300: D loss: 1.7167; G loss: 0.4283\n",
      "step 45400: D loss: 1.6644; G loss: 0.4339\n",
      "step 45500: D loss: 1.6941; G loss: 0.4350\n",
      "step 45600: D loss: 1.7022; G loss: 0.4363\n",
      "step 45700: D loss: 1.7152; G loss: 0.4284\n",
      "step 45800: D loss: 1.6747; G loss: 0.4443\n",
      "step 45900: D loss: 1.7040; G loss: 0.4307\n",
      "step 46000: D loss: 1.6986; G loss: 0.4457\n",
      "step 46100: D loss: 1.7023; G loss: 0.4301\n",
      "step 46200: D loss: 1.6911; G loss: 0.4399\n",
      "step 46300: D loss: 1.7100; G loss: 0.4388\n",
      "step 46400: D loss: 1.7149; G loss: 0.4401\n",
      "step 46500: D loss: 1.6612; G loss: 0.4428\n",
      "step 46600: D loss: 1.6958; G loss: 0.4313\n",
      "step 46700: D loss: 1.6886; G loss: 0.4443\n",
      "step 46800: D loss: 1.6835; G loss: 0.4352\n",
      "step 46900: D loss: 1.7055; G loss: 0.4372\n",
      "step 47000: D loss: 1.6627; G loss: 0.4414\n",
      "step 47100: D loss: 1.7024; G loss: 0.4401\n",
      "step 47200: D loss: 1.6742; G loss: 0.4462\n",
      "step 47300: D loss: 1.6899; G loss: 0.4380\n",
      "step 47400: D loss: 1.6815; G loss: 0.4347\n",
      "step 47500: D loss: 1.7017; G loss: 0.4369\n",
      "step 47600: D loss: 1.7232; G loss: 0.4381\n",
      "step 47700: D loss: 1.6876; G loss: 0.4363\n",
      "step 47800: D loss: 1.7102; G loss: 0.4326\n",
      "step 47900: D loss: 1.7026; G loss: 0.4447\n",
      "step 48000: D loss: 1.6790; G loss: 0.4404\n",
      "step 48100: D loss: 1.7093; G loss: 0.4415\n",
      "step 48200: D loss: 1.6741; G loss: 0.4401\n",
      "step 48300: D loss: 1.6977; G loss: 0.4449\n",
      "step 48400: D loss: 1.6635; G loss: 0.4431\n",
      "step 48500: D loss: 1.7180; G loss: 0.4470\n",
      "step 48600: D loss: 1.6962; G loss: 0.4359\n",
      "step 48700: D loss: 1.6692; G loss: 0.4372\n",
      "step 48800: D loss: 1.6704; G loss: 0.4443\n",
      "step 48900: D loss: 1.6905; G loss: 0.4379\n",
      "step 49000: D loss: 1.7151; G loss: 0.4354\n",
      "step 49100: D loss: 1.6875; G loss: 0.4376\n",
      "step 49200: D loss: 1.6830; G loss: 0.4321\n",
      "step 49300: D loss: 1.6855; G loss: 0.4357\n",
      "step 49400: D loss: 1.6946; G loss: 0.4380\n",
      "step 49500: D loss: 1.6994; G loss: 0.4366\n",
      "step 49600: D loss: 1.6946; G loss: 0.4383\n",
      "step 49700: D loss: 1.6938; G loss: 0.4351\n",
      "step 49800: D loss: 1.7342; G loss: 0.4373\n",
      "step 49900: D loss: 1.7189; G loss: 0.4367\n",
      "step 50000: D loss: 1.6877; G loss: 0.4352\n",
      "step 50100: D loss: 1.6761; G loss: 0.4404\n",
      "step 50200: D loss: 1.6791; G loss: 0.4448\n",
      "step 50300: D loss: 1.6869; G loss: 0.4405\n",
      "step 50400: D loss: 1.7178; G loss: 0.4352\n",
      "step 50500: D loss: 1.7070; G loss: 0.4428\n",
      "step 50600: D loss: 1.6923; G loss: 0.4372\n",
      "step 50700: D loss: 1.6686; G loss: 0.4391\n",
      "step 50800: D loss: 1.6500; G loss: 0.4330\n",
      "step 50900: D loss: 1.6729; G loss: 0.4346\n",
      "step 51000: D loss: 1.6851; G loss: 0.4355\n",
      "step 51100: D loss: 1.6887; G loss: 0.4429\n",
      "step 51200: D loss: 1.6991; G loss: 0.4380\n",
      "step 51300: D loss: 1.7420; G loss: 0.4391\n",
      "step 51400: D loss: 1.6958; G loss: 0.4391\n",
      "step 51500: D loss: 1.7091; G loss: 0.4351\n",
      "step 51600: D loss: 1.6637; G loss: 0.4410\n",
      "step 51700: D loss: 1.6640; G loss: 0.4394\n",
      "step 51800: D loss: 1.7108; G loss: 0.4391\n",
      "step 51900: D loss: 1.7056; G loss: 0.4309\n",
      "step 52000: D loss: 1.6917; G loss: 0.4393\n",
      "step 52100: D loss: 1.6773; G loss: 0.4340\n",
      "step 52200: D loss: 1.7091; G loss: 0.4365\n",
      "step 52300: D loss: 1.6911; G loss: 0.4316\n",
      "step 52400: D loss: 1.7304; G loss: 0.4386\n",
      "step 52500: D loss: 1.6824; G loss: 0.4404\n",
      "step 52600: D loss: 1.6779; G loss: 0.4381\n",
      "step 52700: D loss: 1.7396; G loss: 0.4353\n",
      "step 52800: D loss: 1.6637; G loss: 0.4405\n",
      "step 52900: D loss: 1.6775; G loss: 0.4411\n",
      "step 53000: D loss: 1.6965; G loss: 0.4415\n",
      "step 53100: D loss: 1.7232; G loss: 0.4431\n",
      "step 53200: D loss: 1.7095; G loss: 0.4302\n",
      "step 53300: D loss: 1.7042; G loss: 0.4339\n",
      "step 53400: D loss: 1.6997; G loss: 0.4339\n",
      "step 53500: D loss: 1.6846; G loss: 0.4387\n",
      "step 53600: D loss: 1.7132; G loss: 0.4471\n",
      "step 53700: D loss: 1.7072; G loss: 0.4433\n",
      "step 53800: D loss: 1.6919; G loss: 0.4405\n",
      "step 53900: D loss: 1.6962; G loss: 0.4366\n",
      "step 54000: D loss: 1.6659; G loss: 0.4291\n",
      "step 54100: D loss: 1.6939; G loss: 0.4390\n",
      "step 54200: D loss: 1.7445; G loss: 0.4402\n",
      "step 54300: D loss: 1.7200; G loss: 0.4325\n",
      "step 54400: D loss: 1.6739; G loss: 0.4434\n",
      "step 54500: D loss: 1.6975; G loss: 0.4241\n",
      "step 54600: D loss: 1.6782; G loss: 0.4417\n",
      "step 54700: D loss: 1.7152; G loss: 0.4336\n",
      "step 54800: D loss: 1.6739; G loss: 0.4400\n",
      "step 54900: D loss: 1.6425; G loss: 0.4336\n",
      "step 55000: D loss: 1.7007; G loss: 0.4372\n",
      "step 55100: D loss: 1.6923; G loss: 0.4397\n",
      "step 55200: D loss: 1.6932; G loss: 0.4461\n",
      "step 55300: D loss: 1.7058; G loss: 0.4438\n",
      "step 55400: D loss: 1.7157; G loss: 0.4355\n",
      "step 55500: D loss: 1.6720; G loss: 0.4372\n",
      "step 55600: D loss: 1.6663; G loss: 0.4421\n",
      "step 55700: D loss: 1.6450; G loss: 0.4472\n",
      "step 55800: D loss: 1.6680; G loss: 0.4432\n",
      "step 55900: D loss: 1.7190; G loss: 0.4429\n",
      "step 56000: D loss: 1.7205; G loss: 0.4417\n",
      "step 56100: D loss: 1.7016; G loss: 0.4436\n",
      "step 56200: D loss: 1.7071; G loss: 0.4339\n",
      "step 56300: D loss: 1.6596; G loss: 0.4349\n",
      "step 56400: D loss: 1.6743; G loss: 0.4300\n",
      "step 56500: D loss: 1.7059; G loss: 0.4432\n",
      "step 56600: D loss: 1.7153; G loss: 0.4479\n",
      "step 56700: D loss: 1.6798; G loss: 0.4422\n",
      "step 56800: D loss: 1.6763; G loss: 0.4335\n",
      "step 56900: D loss: 1.7066; G loss: 0.4392\n",
      "step 57000: D loss: 1.6919; G loss: 0.4395\n",
      "step 57100: D loss: 1.6926; G loss: 0.4430\n",
      "step 57200: D loss: 1.6778; G loss: 0.4391\n",
      "step 57300: D loss: 1.7099; G loss: 0.4444\n",
      "step 57400: D loss: 1.7071; G loss: 0.4418\n",
      "step 57500: D loss: 1.6837; G loss: 0.4429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 57600: D loss: 1.7076; G loss: 0.4473\n",
      "step 57700: D loss: 1.6830; G loss: 0.4370\n",
      "step 57800: D loss: 1.6809; G loss: 0.4390\n",
      "step 57900: D loss: 1.6697; G loss: 0.4335\n",
      "step 58000: D loss: 1.6926; G loss: 0.4419\n",
      "step 58100: D loss: 1.6859; G loss: 0.4390\n",
      "step 58200: D loss: 1.6829; G loss: 0.4423\n",
      "step 58300: D loss: 1.6980; G loss: 0.4358\n",
      "step 58400: D loss: 1.6987; G loss: 0.4390\n",
      "step 58500: D loss: 1.6841; G loss: 0.4332\n",
      "step 58600: D loss: 1.6746; G loss: 0.4321\n",
      "step 58700: D loss: 1.6858; G loss: 0.4360\n",
      "step 58800: D loss: 1.6983; G loss: 0.4311\n",
      "step 58900: D loss: 1.7014; G loss: 0.4401\n",
      "step 59000: D loss: 1.6815; G loss: 0.4356\n",
      "step 59100: D loss: 1.6865; G loss: 0.4321\n",
      "step 59200: D loss: 1.6836; G loss: 0.4351\n",
      "step 59300: D loss: 1.7092; G loss: 0.4380\n",
      "step 59400: D loss: 1.6834; G loss: 0.4449\n",
      "step 59500: D loss: 1.6861; G loss: 0.4402\n",
      "step 59600: D loss: 1.6630; G loss: 0.4395\n",
      "step 59700: D loss: 1.6918; G loss: 0.4366\n",
      "step 59800: D loss: 1.7491; G loss: 0.4372\n",
      "step 59900: D loss: 1.7216; G loss: 0.4449\n",
      "step 60000: D loss: 1.6845; G loss: 0.4448\n",
      "step 60100: D loss: 1.7084; G loss: 0.4466\n",
      "step 60200: D loss: 1.7134; G loss: 0.4493\n",
      "step 60300: D loss: 1.7030; G loss: 0.4361\n",
      "step 60400: D loss: 1.6827; G loss: 0.4425\n",
      "step 60500: D loss: 1.6990; G loss: 0.4377\n",
      "step 60600: D loss: 1.7050; G loss: 0.4399\n",
      "step 60700: D loss: 1.6694; G loss: 0.4402\n",
      "step 60800: D loss: 1.6955; G loss: 0.4397\n",
      "step 60900: D loss: 1.6890; G loss: 0.4306\n",
      "step 61000: D loss: 1.7012; G loss: 0.4411\n",
      "step 61100: D loss: 1.6939; G loss: 0.4347\n",
      "step 61200: D loss: 1.6802; G loss: 0.4398\n",
      "step 61300: D loss: 1.7022; G loss: 0.4334\n",
      "step 61400: D loss: 1.7363; G loss: 0.4389\n",
      "step 61500: D loss: 1.6808; G loss: 0.4457\n",
      "step 61600: D loss: 1.7051; G loss: 0.4364\n",
      "step 61700: D loss: 1.6914; G loss: 0.4475\n",
      "step 61800: D loss: 1.6896; G loss: 0.4324\n",
      "step 61900: D loss: 1.6956; G loss: 0.4375\n",
      "step 62000: D loss: 1.7013; G loss: 0.4356\n",
      "step 62100: D loss: 1.6779; G loss: 0.4433\n",
      "step 62200: D loss: 1.7072; G loss: 0.4338\n",
      "step 62300: D loss: 1.7013; G loss: 0.4335\n",
      "step 62400: D loss: 1.6792; G loss: 0.4381\n",
      "step 62500: D loss: 1.6924; G loss: 0.4371\n",
      "step 62600: D loss: 1.7279; G loss: 0.4369\n",
      "step 62700: D loss: 1.6648; G loss: 0.4337\n",
      "step 62800: D loss: 1.6892; G loss: 0.4373\n",
      "step 62900: D loss: 1.6825; G loss: 0.4333\n",
      "step 63000: D loss: 1.6873; G loss: 0.4346\n",
      "step 63100: D loss: 1.7149; G loss: 0.4428\n",
      "step 63200: D loss: 1.6867; G loss: 0.4293\n",
      "step 63300: D loss: 1.6801; G loss: 0.4386\n",
      "step 63400: D loss: 1.6994; G loss: 0.4394\n",
      "step 63500: D loss: 1.6709; G loss: 0.4312\n",
      "step 63600: D loss: 1.7135; G loss: 0.4444\n",
      "step 63700: D loss: 1.6548; G loss: 0.4331\n",
      "step 63800: D loss: 1.6987; G loss: 0.4368\n",
      "step 63900: D loss: 1.6503; G loss: 0.4389\n",
      "step 64000: D loss: 1.6869; G loss: 0.4364\n",
      "step 64100: D loss: 1.7073; G loss: 0.4372\n",
      "step 64200: D loss: 1.6991; G loss: 0.4389\n",
      "step 64300: D loss: 1.6758; G loss: 0.4412\n",
      "step 64400: D loss: 1.6846; G loss: 0.4363\n",
      "step 64500: D loss: 1.7148; G loss: 0.4371\n",
      "step 64600: D loss: 1.6597; G loss: 0.4363\n",
      "step 64700: D loss: 1.7070; G loss: 0.4352\n",
      "step 64800: D loss: 1.7148; G loss: 0.4424\n",
      "step 64900: D loss: 1.6900; G loss: 0.4380\n",
      "step 65000: D loss: 1.7100; G loss: 0.4443\n",
      "step 65100: D loss: 1.6838; G loss: 0.4328\n",
      "step 65200: D loss: 1.6774; G loss: 0.4344\n",
      "step 65300: D loss: 1.7012; G loss: 0.4352\n",
      "step 65400: D loss: 1.6976; G loss: 0.4384\n",
      "step 65500: D loss: 1.7057; G loss: 0.4346\n",
      "step 65600: D loss: 1.6892; G loss: 0.4417\n",
      "step 65700: D loss: 1.6815; G loss: 0.4389\n",
      "step 65800: D loss: 1.6707; G loss: 0.4401\n",
      "step 65900: D loss: 1.6872; G loss: 0.4430\n",
      "step 66000: D loss: 1.6930; G loss: 0.4350\n",
      "step 66100: D loss: 1.6876; G loss: 0.4422\n",
      "step 66200: D loss: 1.7096; G loss: 0.4457\n",
      "step 66300: D loss: 1.6983; G loss: 0.4354\n",
      "step 66400: D loss: 1.7006; G loss: 0.4363\n",
      "step 66500: D loss: 1.6953; G loss: 0.4415\n",
      "step 66600: D loss: 1.7051; G loss: 0.4366\n",
      "step 66700: D loss: 1.6981; G loss: 0.4417\n",
      "step 66800: D loss: 1.6526; G loss: 0.4240\n",
      "step 66900: D loss: 1.6854; G loss: 0.4369\n",
      "step 67000: D loss: 1.6638; G loss: 0.4351\n",
      "step 67100: D loss: 1.6408; G loss: 0.4365\n",
      "step 67200: D loss: 1.7033; G loss: 0.4437\n",
      "step 67300: D loss: 1.6867; G loss: 0.4362\n",
      "step 67400: D loss: 1.6867; G loss: 0.4382\n",
      "step 67500: D loss: 1.6942; G loss: 0.4361\n",
      "step 67600: D loss: 1.6761; G loss: 0.4394\n",
      "step 67700: D loss: 1.6715; G loss: 0.4467\n",
      "step 67800: D loss: 1.6861; G loss: 0.4390\n",
      "step 67900: D loss: 1.6845; G loss: 0.4383\n",
      "step 68000: D loss: 1.6988; G loss: 0.4325\n",
      "step 68100: D loss: 1.6560; G loss: 0.4350\n",
      "step 68200: D loss: 1.6984; G loss: 0.4407\n",
      "step 68300: D loss: 1.6690; G loss: 0.4386\n",
      "step 68400: D loss: 1.6937; G loss: 0.4342\n",
      "step 68500: D loss: 1.7261; G loss: 0.4348\n",
      "step 68600: D loss: 1.6955; G loss: 0.4402\n",
      "step 68700: D loss: 1.7034; G loss: 0.4402\n",
      "step 68800: D loss: 1.6916; G loss: 0.4481\n",
      "step 68900: D loss: 1.6563; G loss: 0.4334\n",
      "step 69000: D loss: 1.6549; G loss: 0.4417\n",
      "step 69100: D loss: 1.7058; G loss: 0.4332\n",
      "step 69200: D loss: 1.6793; G loss: 0.4458\n",
      "step 69300: D loss: 1.7021; G loss: 0.4458\n",
      "step 69400: D loss: 1.6780; G loss: 0.4527\n",
      "step 69500: D loss: 1.6684; G loss: 0.4287\n",
      "step 69600: D loss: 1.6981; G loss: 0.4356\n",
      "step 69700: D loss: 1.7036; G loss: 0.4382\n",
      "step 69800: D loss: 1.7169; G loss: 0.4356\n",
      "step 69900: D loss: 1.6965; G loss: 0.4308\n",
      "step 70000: D loss: 1.7368; G loss: 0.4439\n",
      "step 70100: D loss: 1.6736; G loss: 0.4412\n",
      "step 70200: D loss: 1.6810; G loss: 0.4413\n",
      "step 70300: D loss: 1.6428; G loss: 0.4411\n",
      "step 70400: D loss: 1.7181; G loss: 0.4425\n",
      "step 70500: D loss: 1.6793; G loss: 0.4437\n",
      "step 70600: D loss: 1.6916; G loss: 0.4391\n",
      "step 70700: D loss: 1.7233; G loss: 0.4327\n",
      "step 70800: D loss: 1.7024; G loss: 0.4358\n",
      "step 70900: D loss: 1.6810; G loss: 0.4393\n",
      "step 71000: D loss: 1.6762; G loss: 0.4367\n",
      "step 71100: D loss: 1.6920; G loss: 0.4327\n",
      "step 71200: D loss: 1.6974; G loss: 0.4429\n",
      "step 71300: D loss: 1.6948; G loss: 0.4425\n",
      "step 71400: D loss: 1.6680; G loss: 0.4410\n",
      "step 71500: D loss: 1.7158; G loss: 0.4388\n",
      "step 71600: D loss: 1.6600; G loss: 0.4396\n",
      "step 71700: D loss: 1.6752; G loss: 0.4434\n",
      "step 71800: D loss: 1.7008; G loss: 0.4285\n",
      "step 71900: D loss: 1.7105; G loss: 0.4369\n",
      "step 72000: D loss: 1.7070; G loss: 0.4427\n",
      "step 72100: D loss: 1.6568; G loss: 0.4443\n",
      "step 72200: D loss: 1.6536; G loss: 0.4318\n",
      "step 72300: D loss: 1.6991; G loss: 0.4389\n",
      "step 72400: D loss: 1.6831; G loss: 0.4366\n",
      "step 72500: D loss: 1.7228; G loss: 0.4513\n",
      "step 72600: D loss: 1.6768; G loss: 0.4467\n",
      "step 72700: D loss: 1.6835; G loss: 0.4377\n",
      "step 72800: D loss: 1.6915; G loss: 0.4453\n",
      "step 72900: D loss: 1.6828; G loss: 0.4391\n",
      "step 73000: D loss: 1.7067; G loss: 0.4320\n",
      "step 73100: D loss: 1.7103; G loss: 0.4359\n",
      "step 73200: D loss: 1.6686; G loss: 0.4320\n",
      "step 73300: D loss: 1.7042; G loss: 0.4377\n",
      "step 73400: D loss: 1.6854; G loss: 0.4410\n",
      "step 73500: D loss: 1.7004; G loss: 0.4368\n",
      "step 73600: D loss: 1.7118; G loss: 0.4373\n",
      "step 73700: D loss: 1.6649; G loss: 0.4352\n",
      "step 73800: D loss: 1.6917; G loss: 0.4325\n",
      "step 73900: D loss: 1.7022; G loss: 0.4392\n",
      "step 74000: D loss: 1.6753; G loss: 0.4394\n",
      "step 74100: D loss: 1.6675; G loss: 0.4335\n",
      "step 74200: D loss: 1.7193; G loss: 0.4383\n",
      "step 74300: D loss: 1.7002; G loss: 0.4466\n",
      "step 74400: D loss: 1.6851; G loss: 0.4418\n",
      "step 74500: D loss: 1.6967; G loss: 0.4364\n",
      "step 74600: D loss: 1.6849; G loss: 0.4310\n",
      "step 74700: D loss: 1.7047; G loss: 0.4415\n",
      "step 74800: D loss: 1.6858; G loss: 0.4303\n",
      "step 74900: D loss: 1.6898; G loss: 0.4424\n",
      "step 75000: D loss: 1.7029; G loss: 0.4368\n",
      "step 75100: D loss: 1.6718; G loss: 0.4258\n",
      "step 75200: D loss: 1.6722; G loss: 0.4296\n",
      "step 75300: D loss: 1.6725; G loss: 0.4322\n",
      "step 75400: D loss: 1.7023; G loss: 0.4412\n",
      "step 75500: D loss: 1.7283; G loss: 0.4365\n",
      "step 75600: D loss: 1.6786; G loss: 0.4380\n",
      "step 75700: D loss: 1.6931; G loss: 0.4295\n",
      "step 75800: D loss: 1.6871; G loss: 0.4332\n",
      "step 75900: D loss: 1.6730; G loss: 0.4393\n",
      "step 76000: D loss: 1.6807; G loss: 0.4407\n",
      "step 76100: D loss: 1.6974; G loss: 0.4305\n",
      "step 76200: D loss: 1.6914; G loss: 0.4395\n",
      "step 76300: D loss: 1.6851; G loss: 0.4389\n",
      "step 76400: D loss: 1.6655; G loss: 0.4381\n",
      "step 76500: D loss: 1.6980; G loss: 0.4312\n",
      "step 76600: D loss: 1.6857; G loss: 0.4313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 76700: D loss: 1.6831; G loss: 0.4364\n",
      "step 76800: D loss: 1.6608; G loss: 0.4384\n",
      "step 76900: D loss: 1.7095; G loss: 0.4366\n",
      "step 77000: D loss: 1.6717; G loss: 0.4397\n",
      "step 77100: D loss: 1.7035; G loss: 0.4480\n",
      "step 77200: D loss: 1.6990; G loss: 0.4371\n",
      "step 77300: D loss: 1.6792; G loss: 0.4430\n",
      "step 77400: D loss: 1.6989; G loss: 0.4400\n",
      "step 77500: D loss: 1.6843; G loss: 0.4339\n",
      "step 77600: D loss: 1.6996; G loss: 0.4407\n",
      "step 77700: D loss: 1.7052; G loss: 0.4358\n",
      "step 77800: D loss: 1.6735; G loss: 0.4376\n",
      "step 77900: D loss: 1.7041; G loss: 0.4440\n",
      "step 78000: D loss: 1.7058; G loss: 0.4415\n",
      "step 78100: D loss: 1.6764; G loss: 0.4335\n",
      "step 78200: D loss: 1.6852; G loss: 0.4405\n",
      "step 78300: D loss: 1.7090; G loss: 0.4364\n",
      "step 78400: D loss: 1.6575; G loss: 0.4372\n",
      "step 78500: D loss: 1.7100; G loss: 0.4507\n",
      "step 78600: D loss: 1.6828; G loss: 0.4391\n",
      "step 78700: D loss: 1.6814; G loss: 0.4393\n",
      "step 78800: D loss: 1.6598; G loss: 0.4525\n",
      "step 78900: D loss: 1.6881; G loss: 0.4497\n",
      "step 79000: D loss: 1.6841; G loss: 0.4322\n",
      "step 79100: D loss: 1.6807; G loss: 0.4395\n",
      "step 79200: D loss: 1.6693; G loss: 0.4304\n",
      "step 79300: D loss: 1.6715; G loss: 0.4378\n",
      "step 79400: D loss: 1.6814; G loss: 0.4425\n",
      "step 79500: D loss: 1.7026; G loss: 0.4415\n",
      "step 79600: D loss: 1.6485; G loss: 0.4365\n",
      "step 79700: D loss: 1.6904; G loss: 0.4293\n",
      "step 79800: D loss: 1.6726; G loss: 0.4369\n",
      "step 79900: D loss: 1.7118; G loss: 0.4336\n",
      "step 80000: D loss: 1.6605; G loss: 0.4336\n",
      "step 80100: D loss: 1.6972; G loss: 0.4396\n",
      "step 80200: D loss: 1.7129; G loss: 0.4420\n",
      "step 80300: D loss: 1.6870; G loss: 0.4416\n",
      "step 80400: D loss: 1.7248; G loss: 0.4392\n",
      "step 80500: D loss: 1.6639; G loss: 0.4384\n",
      "step 80600: D loss: 1.6861; G loss: 0.4362\n",
      "step 80700: D loss: 1.7358; G loss: 0.4372\n",
      "step 80800: D loss: 1.6841; G loss: 0.4412\n",
      "step 80900: D loss: 1.7283; G loss: 0.4326\n",
      "step 81000: D loss: 1.6998; G loss: 0.4427\n",
      "step 81100: D loss: 1.6652; G loss: 0.4378\n",
      "step 81200: D loss: 1.6718; G loss: 0.4324\n",
      "step 81300: D loss: 1.7083; G loss: 0.4371\n",
      "step 81400: D loss: 1.7047; G loss: 0.4434\n",
      "step 81500: D loss: 1.6880; G loss: 0.4368\n",
      "step 81600: D loss: 1.6855; G loss: 0.4399\n",
      "step 81700: D loss: 1.6882; G loss: 0.4439\n",
      "step 81800: D loss: 1.6835; G loss: 0.4395\n",
      "step 81900: D loss: 1.6806; G loss: 0.4424\n",
      "step 82000: D loss: 1.6599; G loss: 0.4341\n",
      "step 82100: D loss: 1.6771; G loss: 0.4396\n",
      "step 82200: D loss: 1.6772; G loss: 0.4431\n",
      "step 82300: D loss: 1.6781; G loss: 0.4328\n",
      "step 82400: D loss: 1.7112; G loss: 0.4391\n",
      "step 82500: D loss: 1.7238; G loss: 0.4364\n",
      "step 82600: D loss: 1.6834; G loss: 0.4350\n",
      "step 82700: D loss: 1.7143; G loss: 0.4393\n",
      "step 82800: D loss: 1.6986; G loss: 0.4448\n",
      "step 82900: D loss: 1.7046; G loss: 0.4318\n",
      "step 83000: D loss: 1.6823; G loss: 0.4311\n",
      "step 83100: D loss: 1.6950; G loss: 0.4330\n",
      "step 83200: D loss: 1.6456; G loss: 0.4363\n",
      "step 83300: D loss: 1.7016; G loss: 0.4402\n",
      "step 83400: D loss: 1.7122; G loss: 0.4312\n",
      "step 83500: D loss: 1.7064; G loss: 0.4305\n",
      "step 83600: D loss: 1.7143; G loss: 0.4418\n",
      "step 83700: D loss: 1.6955; G loss: 0.4325\n",
      "step 83800: D loss: 1.7250; G loss: 0.4304\n",
      "step 83900: D loss: 1.6696; G loss: 0.4437\n",
      "step 84000: D loss: 1.7356; G loss: 0.4332\n",
      "step 84100: D loss: 1.6822; G loss: 0.4451\n",
      "step 84200: D loss: 1.7057; G loss: 0.4400\n",
      "step 84300: D loss: 1.6899; G loss: 0.4507\n",
      "step 84400: D loss: 1.7150; G loss: 0.4341\n",
      "step 84500: D loss: 1.6766; G loss: 0.4277\n",
      "step 84600: D loss: 1.7163; G loss: 0.4342\n",
      "step 84700: D loss: 1.6871; G loss: 0.4381\n",
      "step 84800: D loss: 1.6584; G loss: 0.4439\n",
      "step 84900: D loss: 1.7520; G loss: 0.4368\n",
      "step 85000: D loss: 1.7257; G loss: 0.4418\n",
      "step 85100: D loss: 1.6819; G loss: 0.4324\n",
      "step 85200: D loss: 1.7207; G loss: 0.4369\n",
      "step 85300: D loss: 1.6818; G loss: 0.4343\n",
      "step 85400: D loss: 1.6899; G loss: 0.4435\n",
      "step 85500: D loss: 1.6766; G loss: 0.4325\n",
      "step 85600: D loss: 1.6508; G loss: 0.4425\n",
      "step 85700: D loss: 1.6875; G loss: 0.4390\n",
      "step 85800: D loss: 1.7118; G loss: 0.4363\n",
      "step 85900: D loss: 1.6845; G loss: 0.4447\n",
      "step 86000: D loss: 1.7199; G loss: 0.4496\n",
      "step 86100: D loss: 1.7035; G loss: 0.4367\n",
      "step 86200: D loss: 1.6952; G loss: 0.4411\n",
      "step 86300: D loss: 1.6525; G loss: 0.4390\n",
      "step 86400: D loss: 1.6950; G loss: 0.4316\n",
      "step 86500: D loss: 1.6898; G loss: 0.4384\n",
      "step 86600: D loss: 1.7015; G loss: 0.4433\n",
      "step 86700: D loss: 1.6905; G loss: 0.4328\n",
      "step 86800: D loss: 1.6592; G loss: 0.4417\n",
      "step 86900: D loss: 1.7247; G loss: 0.4494\n",
      "step 87000: D loss: 1.7175; G loss: 0.4415\n",
      "step 87100: D loss: 1.6327; G loss: 0.4308\n",
      "step 87200: D loss: 1.6837; G loss: 0.4352\n",
      "step 87300: D loss: 1.7079; G loss: 0.4396\n",
      "step 87400: D loss: 1.6662; G loss: 0.4346\n",
      "step 87500: D loss: 1.6658; G loss: 0.4416\n",
      "step 87600: D loss: 1.6958; G loss: 0.4288\n",
      "step 87700: D loss: 1.6688; G loss: 0.4430\n",
      "step 87800: D loss: 1.7021; G loss: 0.4329\n",
      "step 87900: D loss: 1.6364; G loss: 0.4416\n",
      "step 88000: D loss: 1.7029; G loss: 0.4377\n",
      "step 88100: D loss: 1.6975; G loss: 0.4391\n",
      "step 88200: D loss: 1.6885; G loss: 0.4447\n",
      "step 88300: D loss: 1.7091; G loss: 0.4381\n",
      "step 88400: D loss: 1.6997; G loss: 0.4426\n",
      "step 88500: D loss: 1.7238; G loss: 0.4402\n",
      "step 88600: D loss: 1.6686; G loss: 0.4400\n",
      "step 88700: D loss: 1.6670; G loss: 0.4372\n",
      "step 88800: D loss: 1.6990; G loss: 0.4423\n",
      "step 88900: D loss: 1.6960; G loss: 0.4342\n",
      "step 89000: D loss: 1.6914; G loss: 0.4288\n",
      "step 89100: D loss: 1.7102; G loss: 0.4379\n",
      "step 89200: D loss: 1.6787; G loss: 0.4424\n",
      "step 89300: D loss: 1.6878; G loss: 0.4333\n",
      "step 89400: D loss: 1.7097; G loss: 0.4457\n",
      "step 89500: D loss: 1.6853; G loss: 0.4392\n",
      "step 89600: D loss: 1.6999; G loss: 0.4312\n",
      "step 89700: D loss: 1.7120; G loss: 0.4309\n",
      "step 89800: D loss: 1.6872; G loss: 0.4373\n",
      "step 89900: D loss: 1.7164; G loss: 0.4445\n",
      "step 90000: D loss: 1.6421; G loss: 0.4310\n",
      "step 90100: D loss: 1.7065; G loss: 0.4446\n",
      "step 90200: D loss: 1.6915; G loss: 0.4361\n",
      "step 90300: D loss: 1.6665; G loss: 0.4351\n",
      "step 90400: D loss: 1.6976; G loss: 0.4323\n",
      "step 90500: D loss: 1.7271; G loss: 0.4413\n",
      "step 90600: D loss: 1.6833; G loss: 0.4484\n",
      "step 90700: D loss: 1.6928; G loss: 0.4417\n",
      "step 90800: D loss: 1.7094; G loss: 0.4460\n",
      "step 90900: D loss: 1.7101; G loss: 0.4397\n",
      "step 91000: D loss: 1.6904; G loss: 0.4364\n",
      "step 91100: D loss: 1.6892; G loss: 0.4411\n",
      "step 91200: D loss: 1.6894; G loss: 0.4388\n",
      "step 91300: D loss: 1.6823; G loss: 0.4411\n",
      "step 91400: D loss: 1.7073; G loss: 0.4361\n",
      "step 91500: D loss: 1.7000; G loss: 0.4374\n",
      "step 91600: D loss: 1.7032; G loss: 0.4431\n",
      "step 91700: D loss: 1.6809; G loss: 0.4520\n",
      "step 91800: D loss: 1.6653; G loss: 0.4446\n",
      "step 91900: D loss: 1.6860; G loss: 0.4361\n",
      "step 92000: D loss: 1.6809; G loss: 0.4278\n",
      "step 92100: D loss: 1.6454; G loss: 0.4332\n",
      "step 92200: D loss: 1.6918; G loss: 0.4455\n",
      "step 92300: D loss: 1.6942; G loss: 0.4293\n",
      "step 92400: D loss: 1.6718; G loss: 0.4387\n",
      "step 92500: D loss: 1.7067; G loss: 0.4349\n",
      "step 92600: D loss: 1.6926; G loss: 0.4460\n",
      "step 92700: D loss: 1.6820; G loss: 0.4357\n",
      "step 92800: D loss: 1.6985; G loss: 0.4384\n",
      "step 92900: D loss: 1.6944; G loss: 0.4324\n",
      "step 93000: D loss: 1.6926; G loss: 0.4353\n",
      "step 93100: D loss: 1.6806; G loss: 0.4388\n",
      "step 93200: D loss: 1.6893; G loss: 0.4381\n",
      "step 93300: D loss: 1.6786; G loss: 0.4365\n",
      "step 93400: D loss: 1.6936; G loss: 0.4410\n",
      "step 93500: D loss: 1.7429; G loss: 0.4314\n",
      "step 93600: D loss: 1.7018; G loss: 0.4433\n",
      "step 93700: D loss: 1.7002; G loss: 0.4385\n",
      "step 93800: D loss: 1.6799; G loss: 0.4428\n",
      "step 93900: D loss: 1.6915; G loss: 0.4430\n",
      "step 94000: D loss: 1.7243; G loss: 0.4444\n",
      "step 94100: D loss: 1.6936; G loss: 0.4313\n",
      "step 94200: D loss: 1.6904; G loss: 0.4336\n",
      "step 94300: D loss: 1.6951; G loss: 0.4382\n",
      "step 94400: D loss: 1.7009; G loss: 0.4429\n",
      "step 94500: D loss: 1.7061; G loss: 0.4302\n",
      "step 94600: D loss: 1.6741; G loss: 0.4341\n",
      "step 94700: D loss: 1.7263; G loss: 0.4381\n",
      "step 94800: D loss: 1.7084; G loss: 0.4507\n",
      "step 94900: D loss: 1.6904; G loss: 0.4402\n",
      "step 95000: D loss: 1.6885; G loss: 0.4430\n",
      "step 95100: D loss: 1.6951; G loss: 0.4400\n",
      "step 95200: D loss: 1.6679; G loss: 0.4410\n",
      "step 95300: D loss: 1.6960; G loss: 0.4389\n",
      "step 95400: D loss: 1.7075; G loss: 0.4333\n",
      "step 95500: D loss: 1.7086; G loss: 0.4389\n",
      "step 95600: D loss: 1.7071; G loss: 0.4333\n",
      "step 95700: D loss: 1.6642; G loss: 0.4395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 95800: D loss: 1.6885; G loss: 0.4393\n",
      "step 95900: D loss: 1.7058; G loss: 0.4312\n",
      "step 96000: D loss: 1.6751; G loss: 0.4450\n",
      "step 96100: D loss: 1.6609; G loss: 0.4368\n",
      "step 96200: D loss: 1.6961; G loss: 0.4363\n",
      "step 96300: D loss: 1.6799; G loss: 0.4334\n",
      "step 96400: D loss: 1.6838; G loss: 0.4372\n",
      "step 96500: D loss: 1.6856; G loss: 0.4373\n",
      "step 96600: D loss: 1.6687; G loss: 0.4431\n",
      "step 96700: D loss: 1.6742; G loss: 0.4305\n",
      "step 96800: D loss: 1.7248; G loss: 0.4354\n",
      "step 96900: D loss: 1.6686; G loss: 0.4422\n",
      "step 97000: D loss: 1.7111; G loss: 0.4445\n",
      "step 97100: D loss: 1.6833; G loss: 0.4442\n",
      "step 97200: D loss: 1.7031; G loss: 0.4351\n",
      "step 97300: D loss: 1.6759; G loss: 0.4418\n",
      "step 97400: D loss: 1.6876; G loss: 0.4410\n",
      "step 97500: D loss: 1.6922; G loss: 0.4318\n",
      "step 97600: D loss: 1.7034; G loss: 0.4447\n",
      "step 97700: D loss: 1.6862; G loss: 0.4348\n",
      "step 97800: D loss: 1.6823; G loss: 0.4388\n",
      "step 97900: D loss: 1.6747; G loss: 0.4418\n",
      "step 98000: D loss: 1.7098; G loss: 0.4434\n",
      "step 98100: D loss: 1.7120; G loss: 0.4425\n",
      "step 98200: D loss: 1.6651; G loss: 0.4399\n",
      "step 98300: D loss: 1.7187; G loss: 0.4442\n",
      "step 98400: D loss: 1.7083; G loss: 0.4356\n",
      "step 98500: D loss: 1.7046; G loss: 0.4333\n",
      "step 98600: D loss: 1.6908; G loss: 0.4303\n",
      "step 98700: D loss: 1.6889; G loss: 0.4385\n",
      "step 98800: D loss: 1.6588; G loss: 0.4423\n",
      "step 98900: D loss: 1.6665; G loss: 0.4372\n",
      "step 99000: D loss: 1.7219; G loss: 0.4369\n",
      "step 99100: D loss: 1.6933; G loss: 0.4403\n",
      "step 99200: D loss: 1.7019; G loss: 0.4417\n",
      "step 99300: D loss: 1.6889; G loss: 0.4331\n",
      "step 99400: D loss: 1.6981; G loss: 0.4354\n",
      "step 99500: D loss: 1.7052; G loss: 0.4332\n",
      "step 99600: D loss: 1.6669; G loss: 0.4313\n",
      "step 99700: D loss: 1.7014; G loss: 0.4386\n",
      "step 99800: D loss: 1.6896; G loss: 0.4317\n",
      "step 99900: D loss: 1.6950; G loss: 0.4337\n"
     ]
    }
   ],
   "source": [
    "for global_step  in range(100000):\n",
    "    # Sample data\n",
    "    z = Variable(torch.randn(mb_size, Z_dim))\n",
    "    X, _ = mnist.train.next_batch(mb_size)\n",
    "    X = Variable(torch.from_numpy(X))\n",
    "\n",
    "    # Dicriminator forward-loss-backward-update\n",
    "    G_sample = G(z)\n",
    "    D_real = D(X)\n",
    "    D_fake = D(G_sample)\n",
    "\n",
    "    D_loss_real = criterion(D_real, ones_label)\n",
    "    D_loss_fake = criterion(D_fake, zeros_label)\n",
    "    D_loss = D_loss_real + D_loss_fake\n",
    "\n",
    "    D_loss.backward()\n",
    "    D_solver.step()\n",
    "\n",
    "    # Housekeeping - reset gradient\n",
    "    D.zero_grad()\n",
    "\n",
    "    # Generator forward-loss-backward-update\n",
    "    z = Variable(torch.randn(mb_size, Z_dim))\n",
    "    G_sample = G(z)\n",
    "    D_fake = D(G_sample)\n",
    "\n",
    "    G_loss = criterion(D_fake, ones_label)\n",
    "\n",
    "    G_loss.backward()\n",
    "    G_solver.step()\n",
    "\n",
    "    # Housekeeping - reset gradient\n",
    "    G.zero_grad()\n",
    "    # ======== DISPLAY PROGRESS ======== #\n",
    "    if global_step % 100 == 0:\n",
    "        print('step {}: D loss: {:.4f}; G loss: {:.4f}'\n",
    "              .format(global_step, D_loss.data.numpy()[0], G_loss.data.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Variable(torch.randn(mb_size, Z_dim))\n",
    "sample = G(z).data.resize_(64, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.from_numpy(sample)\n",
    "out = utils.make_grid(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd509dfded0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fig = plt.figure(figsize=(5, 5))\n",
    "i = out.numpy()\n",
    "plt.imshow(i[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
